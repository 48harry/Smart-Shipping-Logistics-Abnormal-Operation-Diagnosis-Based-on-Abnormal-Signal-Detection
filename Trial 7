import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings('ignore')

# Trial 번호 설정
TRIAL_NUM = 7

# 데이터 로드
print(f"=== Trial {TRIAL_NUM} 시작 (런타임 재시작 후) ===")
print("데이터 로딩 중...")
train = pd.read_csv('/content/drive/MyDrive/dacon_sea/train.csv')
test = pd.read_csv('/content/drive/MyDrive/dacon_sea/test.csv')
submission = pd.read_csv('/content/drive/MyDrive/dacon_sea/sample_submission.csv')

print(f"Train shape: {train.shape}")
print(f"Test shape: {test.shape}")

# 기본 데이터 탐색
print("\n=== 기본 데이터 정보 ===")
print(f"Target 분포:\n{train['target'].value_counts().sort_index()}")
print(f"결측치: {train.isnull().sum().sum()}")

# 피처와 타겟 분리
feature_cols = [col for col in train.columns if col.startswith('X_')]
X = train[feature_cols].copy()
y = train['target'].copy()
X_test = test[feature_cols].copy()

print(f"Original Feature 개수: {len(feature_cols)}")

# 대폭 다른 피처 엔지니어링 (Trial 7: PCA + 클러스터링 + 비율)
print("\n=== 새로운 피처 엔지니어링 시작 ===")

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# 1. PCA 피처 생성 (차원 축소보다는 새로운 피처 생성 목적)
pca = PCA(n_components=10, random_state=42)
X_pca = pca.fit_transform(X)
X_test_pca = pca.transform(X_test)

# PCA 피처를 데이터프레임에 추가
for i in range(10):
    X[f'PCA_{i+1}'] = X_pca[:, i]
    X_test[f'PCA_{i+1}'] = X_test_pca[:, i]

print(f"PCA 피처 10개 추가 완료")

# 2. K-means 클러스터링 피처
kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)
X_cluster = kmeans.fit_predict(X[feature_cols])
X_test_cluster = kmeans.predict(X_test[feature_cols])

X['cluster'] = X_cluster
X_test['cluster'] = X_test_cluster

# 클러스터 중심까지의 거리
cluster_distances = []
test_cluster_distances = []

for i in range(len(X)):
    cluster_id = X_cluster[i]
    distance = np.linalg.norm(X.iloc[i][feature_cols].values - kmeans.cluster_centers_[cluster_id])
    cluster_distances.append(distance)

for i in range(len(X_test)):
    cluster_id = X_test_cluster[i]
    distance = np.linalg.norm(X_test.iloc[i][feature_cols].values - kmeans.cluster_centers_[cluster_id])
    test_cluster_distances.append(distance)

X['cluster_distance'] = cluster_distances
X_test['cluster_distance'] = test_cluster_distances

print(f"클러스터 피처 2개 추가 완료")

# 3. 통계 기반 피처들 (더 체계적으로)
# 분위수 기반 피처
X['q25'] = X[feature_cols].quantile(0.25, axis=1)
X['q75'] = X[feature_cols].quantile(0.75, axis=1)
X['iqr'] = X['q75'] - X['q25']

X_test['q25'] = X_test[feature_cols].quantile(0.25, axis=1)
X_test['q75'] = X_test[feature_cols].quantile(0.75, axis=1)
X_test['iqr'] = X_test['q75'] - X_test['q25']

# 상위/하위 피처들의 합
top_10_features = feature_cols[:10]  # 임의로 앞의 10개
bottom_10_features = feature_cols[-10:]  # 뒤의 10개

X['top10_sum'] = X[top_10_features].sum(axis=1)
X['bottom10_sum'] = X[bottom_10_features].sum(axis=1)
X['top_bottom_ratio'] = X['top10_sum'] / (X['bottom10_sum'] + 1e-8)

X_test['top10_sum'] = X_test[top_10_features].sum(axis=1)
X_test['bottom10_sum'] = X_test[bottom_10_features].sum(axis=1)
X_test['top_bottom_ratio'] = X_test['top10_sum'] / (X_test['bottom10_sum'] + 1e-8)

print(f"통계 피처 6개 추가 완료")

# 4. 원본 중요 피처들 기반 파생 피처 (이전 결과 기반)
important_features = ['X_40', 'X_11', 'X_36', 'X_46']

for i, feat1 in enumerate(important_features):
    for feat2 in important_features[i+1:]:
        X[f'{feat1}_div_{feat2}'] = X[feat1] / (X[feat2] + 1e-8)
        X_test[f'{feat1}_div_{feat2}'] = X_test[feat1] / (X_test[feat2] + 1e-8)

print(f"중요 피처 기반 비율 피처 추가 완료")

print(f"\n최종 피처 개수: {X.shape[1]}")
print(f"새로 추가된 피처들 확인:")
new_features = [col for col in X.columns if col not in feature_cols]
print(f"새 피처들: {new_features}")

# 전처리 - RobustScaler 사용 (이상치에 더 강함)
print("\n데이터 전처리 중 (RobustScaler 사용)...")
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
X_test_scaled = scaler.transform(X_test)

# 피처 선택 (상위 100개만 선택)
print("\n피처 선택 중...")
selector = SelectKBest(score_func=f_classif, k=min(100, X.shape[1]-1))
X_selected = selector.fit_transform(X_scaled, y)
X_test_selected = selector.transform(X_test_scaled)

selected_features = X.columns[selector.get_support()]
print(f"선택된 피처 개수: {len(selected_features)}")
print(f"선택된 주요 피처들: {selected_features[:10].tolist()}")

# XGBoost 모델 (다른 하이퍼파라미터)
xgb_model = XGBClassifier(
    n_estimators=500,
    max_depth=7,
    learning_rate=0.03,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=0.5,
    reg_lambda=0.5,
    random_state=777,  # 다른 시드
    n_jobs=-1,
    eval_metric='mlogloss'
)

# 교차 검증으로 성능 평가
print("\n교차 검증 수행 중...")
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=777)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(cv.split(X_selected, y)):
    X_train, X_val = X_selected[train_idx], X_selected[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # 모델 학습
    xgb_model.fit(X_train, y_train)

    # 예측 및 평가
    y_pred = xgb_model.predict(X_val)
    macro_f1 = f1_score(y_val, y_pred, average='macro')
    cv_scores.append(macro_f1)

    print(f"Fold {fold+1} Macro-F1: {macro_f1:.4f}")

print(f"\n평균 CV Macro-F1: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores)*2:.4f})")

# 전체 데이터로 최종 모델 학습
print("\n최종 XGBoost 모델 학습 중...")
xgb_model.fit(X_selected, y)

# 테스트 예측
test_predictions = xgb_model.predict(X_test_selected)

# 예측 결과 디버깅 정보
print(f"\n=== 예측 결과 디버깅 ===")
print(f"Test predictions shape: {test_predictions.shape}")
print(f"Unique predictions: {np.unique(test_predictions)}")
print(f"First 10 predictions: {test_predictions[:10]}")

# 이전 trial들과 비교용 체크섬
checksum = np.sum(test_predictions) + np.mean(test_predictions)
print(f"예측 체크섬: {checksum:.6f}")

# 제출 파일 생성
submission['target'] = test_predictions
submission.to_csv(f'/content/drive/MyDrive/dacon_sea/submission_trial_{TRIAL_NUM}.csv', index=False)

print(f"\n제출 파일 생성 완료: submission_trial_{TRIAL_NUM}.csv")
print(f"예측 분포:\n{pd.Series(test_predictions).value_counts().sort_index()}")

print(f"\n=== Trial {TRIAL_NUM} 완료 ===")
print("주요 변경사항:")
print("- PCA 피처 10개 추가")
print("- K-means 클러스터 피처 2개 추가")
print("- 분위수 기반 통계 피처 6개 추가")
print("- RobustScaler 사용 (이상치에 강함)")
print("- SelectKBest로 상위 100개 피처 선택")
print("- 완전히 다른 XGBoost 하이퍼파라미터 (lr=0.03, reg=0.5)")
print("- 다른 랜덤 시드 (777)")
print(f"- 최종 선택된 피처 개수: {len(selected_features)}개")
